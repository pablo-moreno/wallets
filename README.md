# ğŸ’° Wallets

[![Deploy](https://github.com/pablo-moreno/wallets/workflows/Docker/badge.svg)](https://github.com/pablo-moreno/wallets/actions/workflows/publish-docker.yml)
[![codecov](https://codecov.io/gh/pablo-moreno/wallets/branch/main/graph/badge.svg?token=67nZVOwX2B)](https://codecov.io/gh/pablo-moreno/wallets)

https://wallets.spookydev.com

## Respuestas prueba tÃ©cnica

**1.Indica cÃ³mo se puede optimizar el rendimiento del servicio de listado de operaciones.**

> El servicio de listado de operaciones mÃ¡s conflictivo es el listado de operaciones de un negocio,
> ya que puede tener potencialmente cientos o miles de operaciones realizadas cada dÃ­a por, mientras 
> que los clientes puede que hagan una o varias al dÃ­a (dependiendo, por supuesto, del tipo de cliente). 
> 
> Una vez analizado esto, se tiene que hacer una query especÃ­fica para este servicio de listado de operaciones 
> de un negocio, con la premisa de que se haga el mÃ­nimo nÃºmero de queries en la base de datos.
> 
> Tendremos que indexar en base de datos los campos sobre los que se podrÃ­a filtrar, como es el estado de la operaciÃ³n,
> el tipo, y el negocio (o, si hiciera falta, tambiÃ©n el cliente) que han realizado la operaciÃ³n.
>
> AdemÃ¡s de esto, se tiene que tener en cuenta relaciones de clave forÃ¡nea a otras tablas e intentar que si las hay se
> haga en una sola query o por lo menos en el mÃ­nimo (en `Django` se pueden usar las funciones `select_related` y `prefetch_related`).
>
> Si ninguna de estas opciones optimizara de forma significativa la consulta a nivel de base de datos, se puede pasar 
> a realizar una desnormalizaciÃ³n de datos, aunque eso aÃ±adirÃ­a complejidad al tener que mantenerse la consistencia 
> de datos.
> 
> Otra cuestiÃ³n a tener en cuenta es la de minimizar la respuesta serializada de datos, pues en cuanto tenemos varios 
> niveles de tablas, Django Rest Framework se vuelve relativamente lento, por lo tanto se tiene que enviar el mÃ­nimo
> de informaciÃ³n necesaria para el cliente, ademÃ¡s de marcar los campos de solo lectura como corresponda, o directamente 
> usar un serializador de solo lectura. MÃ¡s info sobre esto [aquÃ­](https://hakibenita.com/django-rest-framework-slow)

**2. Â¿QuÃ© alternativas planteas en el caso que la base de datos relacional de la aplicaciÃ³n se convierta en un cuello de 
botella por saturaciÃ³n en las operaciones de lectura? Â¿Y para las de escritura?**

> **Operaciones de lectura:**
>
> Podemos aÃ±adir una herramienta de cachÃ© para que ni siquiera llegue a ejecutarse la consulta en la base de datos. 
> En este sentido, tenemos [Varnish](https://varnish-cache.org/) que puede mejorar el tiempo de respuesta hasta 
> 300 veces (segÃºn dicen ellos :sweat_smile:). Con esta herramienta aÃ±adimos complejidad a la infraestructura pero no 
> al cÃ³digo y hay que tener cuidado con el tiempo que se mantiene el cacheo porque se puede dar informaciÃ³n no 
> del todo precisa en todo momento.
>
> Utilizar bases de datos replicadas de solo lectura, de modo que las consultas se realicen en estas y la base de datos
> principal no se vea afectada.
>
> **Operaciones de escritura:**
>
> El punto mÃ¡s crÃ­tico de las operaciones de escritura son las transacciones atÃ³micas, por lo tanto es importante utilizar
> correctamente la consulta `SELECT FOR UPDATE SKIP LOCK` de forma que si una fila estÃ¡ bloqueada en un proceso de 
> escritura no impacte en el resto de filas.
> 

**3. Dicen que las bases de datos relacionales no escalan bien, se me ocurre montar el proyecto con alguna NoSQL, 
Â¿quÃ© me recomiendas?**

> No veo razÃ³n para pensar que una base de datos relacional no escala bien. De hecho Postgres, con su campo JSONField, 
> tiene un rendimiento realmente bueno y en algÃºn benchmark supera incluso a MongoDB con un volumen de datos muy alto.
> 
> Para montar este proyecto en NoSQL lo ideal serÃ­a MongoDB, pero hay que tener en cuenta la [atomicidad de las operaciones](https://docs.mongodb.com/manual/core/write-operations-atomicity/).
> y estudiar bien la estructura del modelo de datos para que sea consistente.
>
> Si queremos hacerlo en Django, tenemos el proyecto [Djongo](https://www.djongomapper.com/) que prÃ¡cticamente te da
> la misma interfaz del ORM de Django con soporte para MongoDB.
>
> Otras opciones pueden ser [PyMongo](https://pymongo.readthedocs.io/en/stable/) o [MongoEngine](http://mongoengine.org/).
>

**4.Â¿QuÃ© tipo de mÃ©tricas y servicios nos pueden ayudar a comprobar que la API y el servidor funcionan correctamente?**

> - **[Sentry](https://sentry.io)**: Manejo de errores y excepciones en entornos de producciÃ³n.
>
> - **[Elasticsearch + Logstash + Kibana](https://www.elastic.co/es/what-is/elk-stack)**: GrÃ¡ficas, logs y estado de la 
> aplicaciÃ³n en general.
>
> - **[Status Page](https://www.atlassian.com/es/software/statuspage)**: Servicio de health checks que ademÃ¡s te permite
> enlazarlo con incidencias en Jira.
>

## ğŸ“– Documentation

[![Netlify Status](https://api.netlify.com/api/v1/badges/3d2a724f-1ac3-40a6-b904-ab25919fba24/deploy-status)](https://app.netlify.com/sites/wallets-docs/deploys)

Full API documentation is hosted in [Netlify](https://wallets-docs.netlify.app)

## ğŸ–¥ï¸ Development

> The environment is ready to develop within an Docker Compose environment

First of all, build everything
```bash
docker-compose build
```

Then you can run the docker compose file

```bash
docker-compose up
```

**Useful commands**

> Run tests with coverage
```bash
docker-compose -f docker-compose.test.yml run wallets pytest --cov
```

> Create migrations
```bash
docker-compose -f docker-compose.test.yml run wallets python manage.py makemigrations
```

> Run migrations
```bash
docker-compose -f docker-compose.test.yml run wallets python manage.py migrate
```

> Collect static files
```bash
docker-compose -f docker-compose.test.yml run wallets python manage.py collectstatic --noinput
```

## ğŸš€ Deployment

Wallets is deployed automatically with Github Actions, in a ğŸ³ Docker Swarm environment

To setup the deployment enviroment, you need to copy the `docker-compose.stack.yml` and the `nginx` directory with this structure:

```bash
docker-compose.yml
nginx/
â”œâ”€â”€ default.conf
â”œâ”€â”€ general.conf
â””â”€â”€ proxy.conf
```

Then you need to setup the environment variables:

**ğŸŒµ Environment variables**

```bash
DATABASE_URL: postgres://admin:development@postgres:5432/wallets
SENTRY_DSN: "sentry dsn"
DJANGO_SETTINGS_MODULE: "config.settings.production"
SECRET_KEY: "secret key"
```

**Deploy stack**

To deploy the stack, you have to run:

```bash
docker stack deploy -c docker-compose.yml wallets
```

**CI/CD Github Settings**

You need to set this Github Secrets in `Settings / Secrets`

```bash
CODECOV_TOKEN: Code coverage token
HOST: Remote server ssh IP address
KEY: Private key
PORT: SSH Port
USERNAME: SSH Username
```
